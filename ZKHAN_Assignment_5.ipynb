{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm3WXtkD+5h6hwTK50giQK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zohkhan/test2/blob/main/ZKHAN_Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zohaib Khan\n",
        "\n",
        "Question # 1\n",
        "\n",
        "Please list all characteristics of big data, (2) use one or two sentences to explain their meanings, and (3) give example(s) for each characteristic.\n",
        "\n",
        "Big data is a collection of data from many different sources and is often describe by five characteristics: volume, value, variety, velocity, and veracity.\n",
        "\n",
        "Volume: the size and amounts of big data that companies manage and analyze\n",
        "\n",
        "Value: the most important “V” from the perspective of the business, the value of big data usually comes from insight discovery and pattern recognition that lead to more effective operations, stronger customer relationships and other clear and quantifiable business benefits\n",
        "\n",
        "Variety: the diversity and range of different data types, including unstructured data, semi-structured data and raw data\n",
        "\n",
        "Velocity: the speed at which companies receive, store and manage data – e.g., the specific number of social media posts or search queries received within a day, hour or other unit of time\n",
        "\n",
        "Veracity: the “truth” or accuracy of data and information assets, which often determines executive-level confidence\n"
      ],
      "metadata": {
        "id": "YvgMdE3CT6cF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question # 2 - 1\n",
        "\n",
        "What are the functions of YARN and HDFS in Hadoop?\n",
        "\n",
        "YARN is a generic job scheduling framework and HDFS is a storage framework.\n",
        "Hadoop Distributed File System (HDFS): the storage system for Hadoop spread out over multiple machines as a means to reduce cost and increase reliability.\n",
        "pache YARN (Yet Another Resource Negotiator) is a resource management layer in Hadoop. YARN came into the picture with the introduction of Hadoop 2.x. It allows various data processing engines such as interactive processing, graph processing, batch processing, and stream processing to run and process data stored in HDFS (Hadoop Distributed File System).\n"
      ],
      "metadata": {
        "id": "9rPpfxjFWfQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question # 2 - 2\n",
        "\n",
        "What are the three operations in the rectangle boxes for the following MapReduce example,\n",
        "\n",
        "b-1 = Mapping\n",
        "b-2 = Schuffling\n",
        "b-3 = Reducing"
      ],
      "metadata": {
        "id": "TVUa_NamXNzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question # 2 - 3\n",
        "\n",
        "If you want to train machine leaning models on massive data with Hadoop cluster system,\n",
        "between MapReduce and Spark, which would like to adapt to implement your models? And\n",
        "Why? \n",
        "\n",
        "I would choose Spark because of the \n",
        "\n",
        "\n",
        "Apache Spark is potentially 100 times faster than Hadoop MapReduce.\n",
        "\n",
        "Apache Spark utilizes RAM and isn’t tied to Hadoop’s two-stage paradigm.\n",
        "\n",
        "Apache Spark works well for smaller data sets that can all fit into a server's RAM.\n",
        "\n",
        "Hadoop is more cost-effective for processing massive data sets.\n",
        "\n",
        "Apache Spark is now more popular than Hadoop MapReduce."
      ],
      "metadata": {
        "id": "PnsAnRE4X0Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3\n",
        "\n",
        "Contrast the computational model in Spark to the computational model in Hadoop, i.e., MapReduce. Include a discussion of their relative strengths and weaknesses.\n",
        "\n",
        "The main difference between Hadoop and Spark is the computational model. A computational model is the algorithm and the set of allowable operations to process the data. Hadoop uses the map/reduce. \n",
        "data will be processed and indexed on a key/value base. This processing is done by the map task.\n",
        "Then the data will be shuffled and sorted among the nodes, based on the keys. So that each node contains all values for a particular key.\n",
        "The reduce task will do computations for all the values of the keys (for instance count the total values of a key) and write these to disk.\n",
        "With the Hadoop computational model, there are only two functions available: map and reduce.\n",
        "\n",
        "Spark uses RDD, also called Resilient Distributed Datasets. Working and processing data with RDD is much easier:\n",
        "Reading input data and thereby creating an RDD.\n",
        "Transforming data to new RDDs (by each iteration and in memory). Each transformation of data results in a new RDD. For transforming RDD’s there are lots of functions one can use, like map, flatMap, filter, distinct, sample, union, intersection, subtract, etc. With map/reduce you only have the map-function. (..)\n",
        "Calling operations to compute a result (output data). Again there are lots of actions available, like collect, count, take, top, reduce, fold, etc instead of only reduce with the map/reduce.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LHfnSp0feDuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back to your work/study, based on your own data science practice, which planforms (Spark or Hadoop/MapReduce) would you choose? And Why? \n",
        "\n",
        "The choice depends completely on your business needs. If you are focusing on performance, data compatibility, and ease-of-use, Spark is better than Hadoop. Whereas, Hadoop big data framework is better when you focus on architecture, security, and cost-effectiveness."
      ],
      "metadata": {
        "id": "3ldfFYhvfrlP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79KGh_wLfvLa",
        "outputId": "f7d28429-4287-40b8-9336-94e5e2d5353e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: spark_version=3.0.1\n",
            "env: hadoop_version=2.7\n",
            "env: jdk_version=8u222-b10\n",
            "Python 3.7.15\n"
          ]
        }
      ],
      "source": [
        "%env spark_version=3.0.1\n",
        "%env hadoop_version=2.7\n",
        "%env jdk_version=8u222-b10\n",
        "\n",
        "# innstall java\n",
        "!wget -q https://github.com/AdoptOpenJDK/openjdk8-binaries/releases/download/jdk${jdk_version}/OpenJDK8U-jdk_x64_linux_hotspot_${jdk_version/-/}.tar.gz\n",
        "!tar -xf OpenJDK8U-jdk_x64_linux_hotspot_${jdk_version/-/}.tar.gz\n",
        "\n",
        "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# install spark (change the version number if needed)\n",
        "#!wget -q https://www-us.apache.org/dist/spark/spark-${spark_version}/spark-${spark_version}-bin-hadoop${hadoop_version}.tgz\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-${spark_version}/spark-${spark_version}-bin-hadoop${hadoop_version}.tgz\n",
        "!tar xf spark-${spark_version}-bin-hadoop${hadoop_version}.tgz\n",
        "\n",
        "!python --version\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "print(current_directory)\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"{}/jdk{}\".format(current_directory, os.environ[\"jdk_version\"])   #\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"{}/spark-{}-bin-hadoop{}\".format(current_directory, os.environ[\"spark_version\"], os.environ[\"hadoop_version\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "799asBtOf7G3",
        "outputId": "752dfaf8-25ba-4465-ec13-1d3fddd49ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlB9r-pVgfaI",
        "outputId": "a5d7ae37-b638-42fa-8a4e-023c018c14ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 21 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 30.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=59f501866e7c95af1d23d331f3b3b4adaefc90ab3e67bdae5b13e100a23efced\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init() # os.environ[\"SPARK_HOME\"]\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "# create SparkConf\n",
        "conf = SparkConf().setAppName('pyspark-app').setMaster('local[*]')\n",
        "# create SparkSession instance\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "lJ25dvBFlLym",
        "outputId": "e0605678-be08-4f8d-95a2-2834d546470e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7febe3651dd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://60697855b913:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-app</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/\"\n",
        "parkinsons_file = \"parkinsons.data\"\n",
        "\n",
        "r = requests.get(url + parkinsons_file)\n",
        "open(parkinsons_file, 'wb').write(r.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rDN0TEzm3k2",
        "outputId": "c10d0a8d-2c9e-435f-812b-36c1fb4ab379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40697"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "  \n",
        "spark = SparkSession.builder.appName(parkinsons_file).getOrCreate()\n",
        "\n",
        "df = spark.read.text(parkinsons_file)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK7MPrI89AWy",
        "outputId": "44fd2f82-136f-4d74-814f-c62cd52ce5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[value: string]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(parkinsons_file, header=True, inferSchema=True)\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3QbbCS2nO76",
        "outputId": "4db29b67-608b-4824-9198-5d21a16adde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+------------+------------+--------------+----------------+--------+--------+----------+------------+----------------+------------+------------+--------+-----------+-------+------+------+--------+--------+---------+--------+--------+--------+\n",
            "|          name|MDVP:Fo(Hz)|MDVP:Fhi(Hz)|MDVP:Flo(Hz)|MDVP:Jitter(%)|MDVP:Jitter(Abs)|MDVP:RAP|MDVP:PPQ|Jitter:DDP|MDVP:Shimmer|MDVP:Shimmer(dB)|Shimmer:APQ3|Shimmer:APQ5|MDVP:APQ|Shimmer:DDA|    NHR|   HNR|status|    RPDE|     DFA|  spread1| spread2|      D2|     PPE|\n",
            "+--------------+-----------+------------+------------+--------------+----------------+--------+--------+----------+------------+----------------+------------+------------+--------+-----------+-------+------+------+--------+--------+---------+--------+--------+--------+\n",
            "|phon_R01_S01_1|    119.992|     157.302|      74.997|       0.00784|          7.0E-5|  0.0037| 0.00554|   0.01109|     0.04374|           0.426|     0.02182|      0.0313| 0.02971|    0.06545|0.02211|21.033|     1|0.414783|0.815285|-4.813031|0.266482|2.301442|0.284654|\n",
            "|phon_R01_S01_2|      122.4|      148.65|     113.819|       0.00968|          8.0E-5| 0.00465| 0.00696|   0.01394|     0.06134|           0.626|     0.03134|     0.04518| 0.04368|    0.09403|0.01929|19.085|     1|0.458359|0.819521|-4.075192| 0.33559|2.486855|0.368674|\n",
            "|phon_R01_S01_3|    116.682|     131.111|     111.555|        0.0105|          9.0E-5| 0.00544| 0.00781|   0.01633|     0.05233|           0.482|     0.02757|     0.03858|  0.0359|     0.0827|0.01309|20.651|     1|0.429895|0.825288|-4.443179|0.311173|2.342259|0.332634|\n",
            "|phon_R01_S01_4|    116.676|     137.871|     111.366|       0.00997|          9.0E-5| 0.00502| 0.00698|   0.01505|     0.05492|           0.517|     0.02924|     0.04005| 0.03772|    0.08771|0.01353|20.644|     1|0.434969|0.819235|-4.117501|0.334147|2.405554|0.368975|\n",
            "|phon_R01_S01_5|    116.014|     141.781|     110.655|       0.01284|          1.1E-4| 0.00655| 0.00908|   0.01966|     0.06425|           0.584|      0.0349|     0.04825| 0.04465|     0.1047|0.01767|19.649|     1|0.417356|0.823484|-3.747787|0.234513| 2.33218|0.410335|\n",
            "+--------------+-----------+------------+------------+--------------+----------------+--------+--------+----------+------------+----------------+------------+------------+--------+-----------+-------+------+------+--------+--------+---------+--------+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.schema.fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPwupnMAj1WX",
        "outputId": "7eee4ad6-7aa7-4e7e-ec11-3b51bf97b450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StructField(name,StringType,true),\n",
              " StructField(MDVP:Fo(Hz),DoubleType,true),\n",
              " StructField(MDVP:Fhi(Hz),DoubleType,true),\n",
              " StructField(MDVP:Flo(Hz),DoubleType,true),\n",
              " StructField(MDVP:Jitter(%),DoubleType,true),\n",
              " StructField(MDVP:Jitter(Abs),DoubleType,true),\n",
              " StructField(MDVP:RAP,DoubleType,true),\n",
              " StructField(MDVP:PPQ,DoubleType,true),\n",
              " StructField(Jitter:DDP,DoubleType,true),\n",
              " StructField(MDVP:Shimmer,DoubleType,true),\n",
              " StructField(MDVP:Shimmer(dB),DoubleType,true),\n",
              " StructField(Shimmer:APQ3,DoubleType,true),\n",
              " StructField(Shimmer:APQ5,DoubleType,true),\n",
              " StructField(MDVP:APQ,DoubleType,true),\n",
              " StructField(Shimmer:DDA,DoubleType,true),\n",
              " StructField(NHR,DoubleType,true),\n",
              " StructField(HNR,DoubleType,true),\n",
              " StructField(status,IntegerType,true),\n",
              " StructField(RPDE,DoubleType,true),\n",
              " StructField(DFA,DoubleType,true),\n",
              " StructField(spread1,DoubleType,true),\n",
              " StructField(spread2,DoubleType,true),\n",
              " StructField(D2,DoubleType,true),\n",
              " StructField(PPE,DoubleType,true)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehFr67ImRNQm",
        "outputId": "8530af40-cbee-4c83-9351-73f113b7baab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+------------------+-----------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+-------------------+\n",
            "|summary|          name|       MDVP:Fo(Hz)|     MDVP:Fhi(Hz)|      MDVP:Flo(Hz)|      MDVP:Jitter(%)|    MDVP:Jitter(Abs)|            MDVP:RAP|            MDVP:PPQ|          Jitter:DDP|        MDVP:Shimmer|   MDVP:Shimmer(dB)|        Shimmer:APQ3|        Shimmer:APQ5|            MDVP:APQ|         Shimmer:DDA|                 NHR|               HNR|            status|               RPDE|                DFA|           spread1|            spread2|                 D2|                PPE|\n",
            "+-------+--------------+------------------+-----------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+-------------------+\n",
            "|  count|           195|               195|              195|               195|                 195|                 195|                 195|                 195|                 195|                 195|                195|                 195|                 195|                 195|                 195|                 195|               195|               195|                195|                195|               195|                195|                195|                195|\n",
            "|   mean|          null|154.22864102564105| 197.104917948718|116.32463076923077|0.006220461538461542|4.395897435897438E-5|0.003306410256410...|0.003446358974358...|0.009919948717948712|  0.0297091282051282| 0.2822512820512821|0.015664153846153845|0.017878256410256418| 0.02408148717948718| 0.04699261538461537|0.024847076923076923|21.885974358974366|0.7538461538461538|0.49853553846153836| 0.7180990461538465|-5.684396743589743|0.22651034871794856| 2.3818260871794874|0.20655164102564105|\n",
            "| stddev|          null| 41.39006474907147|91.49154763503034| 43.52141318199366|0.004848133692602559|3.482190859976327E-5|0.002967774416201...| 0.00275897664696793|0.008903344355858987|0.018856931858946806|0.19487729006053411|0.010153161595709014|0.012023705538741724|0.016946736247029436|0.030459119431240404|0.040418448556069284| 4.425764269063426|0.4318780337122647|0.10394171413073464|0.05533583034659674| 1.090207763740309|0.08340576262039777|0.38279904654611674|0.09011932248227507|\n",
            "|    min|phon_R01_S01_1|            88.333|          102.145|            65.476|             0.00168|              7.0E-6|              6.8E-4|              9.2E-4|             0.00204|             0.00954|              0.085|             0.00455|              0.0057|             0.00719|             0.01364|              6.5E-4|             8.441|                 0|            0.25657|           0.574282|         -7.964984|           0.006274|           1.423287|           0.044539|\n",
            "|    max|phon_R01_S50_6|           260.105|           592.03|            239.17|             0.03316|              2.6E-4|             0.02144|             0.01958|             0.06433|             0.11908|              1.302|             0.05647|              0.0794|             0.13778|             0.16942|             0.31482|            33.047|                 1|           0.685151|           0.825288|         -2.434031|           0.450493|           3.671155|           0.527367|\n",
            "+-------+--------------+------------------+-----------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}